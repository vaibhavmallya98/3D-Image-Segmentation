{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614c25b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 22:12:02.824570: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-05 22:12:02.870646: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 48115676\n",
      "Total number of trainable parameters: 16495826\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "from timm.models.layers import trunc_normal_\n",
    "import pytorch_lightning as pl \n",
    "import monai\n",
    "#import pytorch_lightning as pl\n",
    "from monai import transforms\n",
    "from monai.data import Dataset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from monai.config import KeysCollection\n",
    "from monai.losses import DiceLoss, DiceFocalLoss, DiceCELoss\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric, ConfusionMatrixMetric, MeanIoU\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.utils import set_determinism\n",
    "from monai.data import decollate_batch\n",
    "#from monai.data import NiftiSaver, write_nifti\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Resized,\n",
    "    Spacingd,\n",
    "    ScaleIntensityd,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    ConvertToMultiChannelBasedOnBratsClassesd,\n",
    "    SpatialPadd,\n",
    "    ScaleIntensityRangePercentilesd,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "#IMPORT THE RES SWIN UNET MODEL \n",
    "\n",
    "from ResSwinUnet import swinUnet_t_3D_Residual\n",
    "\n",
    "from SwinUnet_3D import swinUnet_t_3D\n",
    "\n",
    "from monai.networks.nets import UNETR, UNet, VNet, DynUNet, SegResNet, SwinUNETR, AttentionUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570dfc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcam import medcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3a6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "#from celluloid import Camera\n",
    "#import torchio as tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a40f0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = './scratch/MICCAI_BraTS2020_TrainingData'\n",
    "\n",
    "sample_path = './scratch/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355'\n",
    "\n",
    "sample_scan_paths = os.listdir(sample_path)\n",
    "\n",
    "\n",
    "img_names = ['mask','t2','flair','t1ce','t1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92d012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING PATHS OF ALL 4 MODALITIES AND THE GROUND TRUTH MASK IN SEPARATE LISTS \n",
    "\n",
    "flair_paths = []\n",
    "t1_paths = []\n",
    "t1ce_paths = []\n",
    "t2_paths = []\n",
    "seg_paths = []\n",
    "\n",
    "for i in os.listdir(dataset_path):\n",
    "    \n",
    "    \n",
    "    for j in os.listdir(os.path.join(dataset_path, i)):\n",
    "        if(j.endswith('flair.nii.gz')):\n",
    "            flair_paths.append(os.path.join(dataset_path, i, j))\n",
    "\n",
    "        elif(j.endswith('t1.nii.gz')):\n",
    "            t1_paths.append(os.path.join(dataset_path, i, j))\n",
    "\n",
    "        elif(j.endswith('t1ce.nii.gz')):\n",
    "            t1ce_paths.append(os.path.join(dataset_path, i, j))\n",
    "\n",
    "        elif(j.endswith('t2.nii.gz')):\n",
    "            t2_paths.append(os.path.join(dataset_path, i, j))\n",
    "\n",
    "        elif(j.endswith('seg.nii.gz')):\n",
    "            seg_paths.append(os.path.join(dataset_path, i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b6e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE WANT THE INPUT TO BE FED AS A FOUR CHANNEL IMAGE WHICH CONTAINS ALL 4 MODALITIES \n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for i in range(len(flair_paths)):\n",
    "    \n",
    "    flair_path = flair_paths[i]\n",
    "    t1_path = t1_paths[i]\n",
    "    t1ce_path = t1ce_paths[i]\n",
    "    t2_path = t2_paths[i]\n",
    "    seg_path = seg_paths[i]\n",
    "    \n",
    "    files = [flair_path,t1_path,t1ce_path,t2_path]\n",
    "    \n",
    "    train_x.append(files)\n",
    "    train_y.append(seg_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdfa373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING A DICTIONARY TO SAVE ALL IMAGES AND LABELS \n",
    "\n",
    "train_dict = []\n",
    "\n",
    "for x, y in zip(train_x, train_y):\n",
    "    info = {'image': x, 'label': y}\n",
    "    train_dict.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56eb8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 \n",
    "spacings = [1.0, 1.0, 1.0]\n",
    "\n",
    "#REGION OF INTEREST SIZE = 128*128*128 \n",
    "RoiSize = [128, 128, 128]  \n",
    "\n",
    "#SLIDING WINDOW SIZE = 4*4*4\n",
    "window_size = [it // 32 for it in RoiSize]  \n",
    "\n",
    "\n",
    "overlap = 0.125\n",
    "\n",
    "#SINCE WE ARE USING ALL 4 MODALITIES, THE INPUT CHANNELS WILL BE 4\n",
    "in_channels = 4\n",
    "\n",
    "l_percent = 0.5\n",
    "u_percent = 99.5\n",
    "\n",
    "train_ratio, val_ratio, test_ratio = [0.8, 0.2, 0.0]\n",
    "\n",
    "#VARY BATCH SIZE AND THREADS (WORKERS) ACCORDING TO THE CAPACITY OF YOUR MACHINE  \n",
    "\n",
    "BatchSize = 1\n",
    "NumWorkers = 4  \n",
    "\n",
    "#CHANGE MAX EPOCH AND MIN EPOCH ACCORDINGLY\n",
    "\n",
    "max_epoch = 260\n",
    "min_epoch = 250\n",
    "\n",
    "'''\n",
    "The t_max parameter in the Cosine Annealing Learning Rate (LR) scheduler refers to the maximum number of iterations \n",
    "(or epochs) after which the learning rate will complete one cosine cycle.\n",
    "\n",
    "In Cosine Annealing, the learning rate starts at a high value and decreases following a cosine curve until it reaches \n",
    "a minimum value (often very close to 0). Once the t_max number of iterations is reached, \n",
    "the scheduler restarts the process, and the learning rate begins increasing again.\n",
    "'''\n",
    "\n",
    "LRCycle = 10 #tmax parameter to be used for cosine annealing learning rate scheduler \n",
    "'''\n",
    "ED (peritumoral edema) =  2,ET (enhancing tumor) =  4, \n",
    "NET (non-enhancing tumor)  1, 0- NOT TUMOR \n",
    "WT = ED + ET + NET\n",
    "TC = ET+NET\n",
    "ET\n",
    "'''\n",
    "n_classes = 3  #TC, ET, WT\n",
    "\n",
    "lr = 3e-4  \n",
    "\n",
    "# ValidSegDir = os.path.join(data_path, 'ValidSeg', model_name)\n",
    "# PredDataDir = os.path.join(data_path, 'Brats2021Pred')\n",
    "# PredSegDir = os.path.join(data_path, 'PredSeg', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c102f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "WT = ED + ET + NET\n",
    "TC = ET+NET\n",
    "ET\n",
    "\n",
    "Below is the class for computing WT, TC and ET \n",
    "'''\n",
    "\n",
    "class ReverseBratsLabel(transforms.Transform):\n",
    "    def __call__(self, pred):\n",
    "        pred = pred.bool()\n",
    "        tc = pred[0, ::]\n",
    "        wt = pred[1, ::]\n",
    "        et = pred[2, ::]\n",
    "        res = torch.zeros(pred.shape[1:])\n",
    "\n",
    "        # label_4 = et\n",
    "        # label_2 = wt - tc\n",
    "        # label_1 = tc - et\n",
    "\n",
    "        label_4 = et\n",
    "        label_2 = torch.logical_and(wt, torch.logical_not(tc))\n",
    "        label_1 = torch.logical_and(tc, torch.logical_not(et))\n",
    "\n",
    "        res[label_1] = 1\n",
    "        res[label_2] = 2\n",
    "        res[label_4] = 4\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a321120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms for training data \n",
    "\n",
    "train_process = Compose(\n",
    "            [\n",
    "                # load 4 Nifti images and stack them together\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                \n",
    "                #ensuring that the channels are the first dimension for image and label \n",
    "                EnsureChannelFirstd(keys=\"image\"),\n",
    "                ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "\n",
    "                #ensuring that both image and labels are of the same orientation\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(keys=[\"image\", \"label\"], pixdim=spacings,\n",
    "                         mode=(\"bilinear\", \"nearest\"), ),\n",
    "                 \n",
    "\n",
    "                RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=RoiSize, random_size=False),\n",
    "                \n",
    "                \n",
    "                #performing flipping with probability of 0.5 along all 3 axes \n",
    "                RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "                RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "                RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "\n",
    "                NormalizeIntensityd(keys=\"image\", nonzero=True,\n",
    "                                    channel_wise=True),\n",
    "                RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "                RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "\n",
    "                EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d70d4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms for validation data \n",
    "\n",
    "val_process = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                EnsureChannelFirstd(keys=\"image\"),\n",
    "                ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=spacings,\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                #RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=RoiSize, random_size=False),\n",
    "                #Resized(keys=['image','label'], spatial_size=(128,128,128), mode=['area','nearest']),\n",
    "                #ScaleIntensityd(keys='image', minv=0.0, maxv=1.0), \n",
    "                NormalizeIntensityd(keys=\"image\", nonzero=True,\n",
    "                                    channel_wise=True),\n",
    "                EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ac5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms for test data \n",
    "\n",
    "test_process = Compose([\n",
    "            LoadImaged(keys=['image','label']),\n",
    "            EnsureChannelFirstd(keys='image'),\n",
    "            ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            Orientationd(keys=['image','label'], axcodes=\"RAS\"),\n",
    "            Spacingd(keys=[\"image\",\"label\"], pixdim=(1.0, 1.0, 1.0), mode=(\"bilinear\", \"nearest\")),\n",
    "            #RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=RoiSize, random_size=False),\n",
    "            #Resized(keys=['image','label'], spatial_size=(128,128,128), mode=['area','nearest']),\n",
    "            #ScaleIntensityd(keys='image', minv=0.0, maxv=1.0), \n",
    "            #NormalizeIntensityd(keys='image', nonzero=True, channel_wise=True),\n",
    "            EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "340af902",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(train_dict)\n",
    "train_num = int(num * train_ratio) #0.8\n",
    "val_test_num = int(num * val_ratio) #0.2\n",
    "if train_num + val_test_num != num: \n",
    "    remain = num - train_num - val_test_num\n",
    "    val_test_num += remain\n",
    "    \n",
    "    \n",
    "train_dict, val_dict = random_split(train_dict, [train_num, val_test_num],\n",
    "                                    generator=torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5340893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(train_dict, transform=train_process)\n",
    "val_set = Dataset(val_dict, transform=val_process)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BatchSize, num_workers=NumWorkers, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BatchSize, num_workers=NumWorkers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90fb9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = swinUnet_t_3D_Residual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7b33f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING THE DATA MODULE WHICH UTILIZES THE DATA LOADERS \n",
    "\n",
    "class BratsDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f69a4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING THE MODULE WHICH HANDLES THE TRAINING AND VALIDATION PROCESS\n",
    "\n",
    "class BratsModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BratsModel, self).__init__()\n",
    "        \n",
    "        #index of model name \n",
    "        #self.index = index\n",
    "        \n",
    "        #self.learning_rate = learning_rate \n",
    "        self.net = model\n",
    "        #self.model_name = model_names[self.index]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        return self.net(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #THIS FUNCTION HANDLES TRAINING FOR EVERY TRAINING BATCH\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #THIS FUNCTION HANDLES VALIDATION FOR EVERY VALIDATION BATCH\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass \n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        pass \n",
    "        \n",
    "            \n",
    "            \n",
    "    #THIS FUNCTION LOGS ALL THE METRICS FOR TRAINING AT THE END OF EVERY EPOCH \n",
    "    def on_training_epoch_end(self):\n",
    "        pass \n",
    "            \n",
    "            \n",
    "    #THIS FUNCTION LOGS ALL THE METRICS FOR VALIDATION AT THE END OF EVERY EPOCH\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass \n",
    "            \n",
    "    #THIS FUNCTION HANDLES CALCULATION OF METRICS AT THE END OF EVERY EPOCH\n",
    "\n",
    "    def shared_epoch_end(self, loss_key, stage: int = 0):  #stage - 0 for training and 1 for validation\n",
    "        pass \n",
    "    #THIS FUNCTION HANDLES CALCULATION OF METRICS AFTER READING EVERY SAMPLE DURING TRAINING AND VALIDATION\n",
    "\n",
    "    def shared_step(self, y_hat, y, stage: int = 0):  #stage - 0 for training and 1 for validation\n",
    "        pass \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "933f1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setseed():\n",
    "    pl.seed_everything(seed)\n",
    "    set_determinism(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00a97ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the weights of the pre-trained model \n",
    "#change the path accordingly \n",
    "\n",
    "save_path = './scratch/Validation_Results_3rd_Trial/ResSWINUnet/BraTS_2020/logs/learning_rate_0.0003/epoch=249-valid_loss=0.1616-wt_valid_mean_dice=0.9170-tc_valid_mean_dice=0.8539-et_valid_mean_dice=0.8031.ckpt'\n",
    "\n",
    "saved_model = BratsModel.load_from_checkpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "281f7f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model.net.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b2d7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai.transforms import Compose, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the resize transform\n",
    "resize_transform = Compose([\n",
    "    Resize(spatial_size=(128, 128, 128), mode='trilinear', align_corners=True)\n",
    "])\n",
    "\n",
    "resized_val_loader = []\n",
    "\n",
    "for batch_idx, batch in enumerate(val_loader):\n",
    "    x = batch[\"image\"]\n",
    "    y = batch[\"label\"]\n",
    "    \n",
    "    # Initialize a tensor to hold the resized channels\n",
    "    resized_x = torch.zeros((1, 4, 128, 128, 128), device=x.device)\n",
    "    \n",
    "    # Resize each channel separately\n",
    "    for channel in range(4):\n",
    "        x_channel = x[0, channel, :, :, :]\n",
    "        resized_x[0, channel, :, :, :] = resize_transform(x_channel.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "    # Create a new batch with the resized input\n",
    "    resized_batch = {\"image\": resized_x, \"label\": y}\n",
    "    \n",
    "    resized_val_loader.append(resized_batch)\n",
    "\n",
    "# Convert the list of resized batches to a DataLoader\n",
    "resized_val_loader = DataLoader(resized_val_loader, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01d4e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.visualize import GradCAMpp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Getting the CAMs for enc12, res12, enc3, res3, enc5, res5 layers \n",
    "\n",
    "gcam_enc12 = GradCAMpp(nn_module=saved_model.net, target_layers=\"enc12\")\n",
    "gcam_res12 = GradCAMpp(nn_module=saved_model.net, target_layers=\"res12\")\n",
    "\n",
    "gcam_enc3 = GradCAMpp(nn_module=saved_model.net, target_layers=\"enc3\")\n",
    "gcam_res3 = GradCAMpp(nn_module=saved_model.net, target_layers=\"res3\")\n",
    "\n",
    "gcam_enc5 = GradCAMpp(nn_module=saved_model.net, target_layers=\"enc5\")\n",
    "gcam_res5 = GradCAMpp(nn_module=saved_model.net, target_layers=\"res5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2589a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.visualize import GradCAM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Getting the CAMs for enc12, res12, enc3, res3, enc5, res5 layers \n",
    "\n",
    "gcam_enc12 = GradCAM(nn_module=saved_model.net, target_layers=\"enc12\")\n",
    "gcam_res12 = GradCAM(nn_module=saved_model.net, target_layers=\"res12\")\n",
    "\n",
    "gcam_enc3 = GradCAM(nn_module=saved_model.net, target_layers=\"enc3\")\n",
    "gcam_res3 = GradCAM(nn_module=saved_model.net, target_layers=\"res3\")\n",
    "\n",
    "gcam_enc5 = GradCAM(nn_module=saved_model.net, target_layers=\"enc5\")\n",
    "gcam_res5 = GradCAM(nn_module=saved_model.net, target_layers=\"res5\")\n",
    "\n",
    "for batch_idx,batch in enumerate(resized_val_loader):\n",
    "    images = batch[\"image\"].to('cuda')\n",
    "    images = torch.squeeze(images,dim=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate GradCAM for classes 0,1,2  \n",
    "    cam0_enc12 = gcam_enc12(x=images, class_idx=0)\n",
    "    cam1_enc12 = gcam_enc12(x=images, class_idx=1)\n",
    "    cam2_enc12 = gcam_enc12(x=images, class_idx=2)\n",
    "    \n",
    "    cam0_res12 = gcam_res12(x=images, class_idx=0)\n",
    "    cam1_res12 = gcam_res12(x=images, class_idx=1)\n",
    "    cam2_res12 = gcam_res12(x=images, class_idx=2)\n",
    "    \n",
    "    cam0_enc3 = gcam_enc3(x=images, class_idx=0)\n",
    "    cam1_enc3 = gcam_enc3(x=images, class_idx=1)\n",
    "    cam2_enc3 = gcam_enc3(x=images, class_idx=2)\n",
    "    \n",
    "    cam0_res3 = gcam_res3(x=images, class_idx=0)\n",
    "    cam1_res3 = gcam_res3(x=images, class_idx=1)\n",
    "    cam2_res3 = gcam_res3(x=images, class_idx=2)\n",
    "    \n",
    "    cam0_enc5 = gcam_enc5(x=images, class_idx=0)\n",
    "    cam1_enc5 = gcam_enc5(x=images, class_idx=1)\n",
    "    cam2_enc5 = gcam_enc5(x=images, class_idx=2)\n",
    "    \n",
    "    cam0_res5 = gcam_res5(x=images, class_idx=0)\n",
    "    cam1_res5 = gcam_res5(x=images, class_idx=1)\n",
    "    cam2_res5 = gcam_res5(x=images, class_idx=2)\n",
    "    \n",
    "    cam_images = [cam0_enc12,cam1_enc12,cam2_enc12, \n",
    "              cam0_res12,cam1_res12,cam2_res12,\n",
    "              cam0_enc3,cam1_enc3,cam2_enc3,\n",
    "              cam0_res3,cam1_res3,cam2_res3,\n",
    "              cam0_enc5,cam1_enc5,cam2_enc5,\n",
    "              cam0_res5,cam1_res5,cam2_res5]\n",
    "    \n",
    "    \n",
    "    cam_image_names = [\"CAM0 ENC12\", \"CAM1 ENC12\", \"CAM2 ENC12\",\n",
    "                       \"CAM0 RES12\", \"CAM1 RES12\", \"CAM2 RES12\",\n",
    "                       \"CAM0 ENC3\", \"CAM1 ENC3\", \"CAM2 ENC3\",\n",
    "                       \"CAM0 RES3\", \"CAM1 RES3\", \"CAM2 RES3\",\n",
    "                       \"CAM0 ENC5\", \"CAM1 ENC5\", \"CAM2 ENC5\",\n",
    "                       \"CAM0 RES5\", \"CAM1 RES5\", \"CAM2 RES5\"]\n",
    "    \n",
    "    \n",
    "    #print(cam.shape)\n",
    "    \n",
    "    # Visualize the GradCAM results\n",
    "    # We'll visualize the middle slice of each channel\n",
    "    fig, axes = plt.subplots(2, 7, figsize=(20, 10))\n",
    "    image_names = [\"flair\",\"t1\",\"t1ce\",\"t2\"]\n",
    "    \n",
    "    \n",
    "    for j in range(0,2):\n",
    "        for channel in range(4):\n",
    "        # Visualizing the Original images\n",
    "            axes[j,channel].imshow(images[0, channel, :, :, 57].cpu().numpy(), cmap='gray')\n",
    "            axes[j,channel].set_title(image_names[channel])\n",
    "            axes[j,channel].axis('off')\n",
    "        \n",
    "        #visualizing the CAMs using the jet heatmaps \n",
    "        \n",
    "        for i in range(4,7):\n",
    "            axes[j,i].imshow(cam_images[-4+i+3*j][0, 0, :, :, 57].cpu().numpy(),cmap='jet', alpha=0.7)\n",
    "            axes[j,i].set_title(cam_image_names[-4+i+3*j])\n",
    "            axes[j,i].axis('off')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from monai.visualize import GradCAM\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming 'saved_model' is your loaded model\n",
    "\n",
    "# gcam_enc12 = GradCAM(nn_module=saved_model.net, target_layers=\"enc12\")\n",
    "# gcam_res12 = GradCAM(nn_module=saved_model.net, target_layers=\"res12\")\n",
    "\n",
    "# gcam_enc3 = GradCAM(nn_module=saved_model.net, target_layers=\"enc3\")\n",
    "# gcam_res3 = GradCAM(nn_module=saved_model.net, target_layers=\"res3\")\n",
    "\n",
    "# gcam_enc5 = GradCAM(nn_module=saved_model.net, target_layers=\"enc5\")\n",
    "# gcam_res5 = GradCAM(nn_module=saved_model.net, target_layers=\"res5\")\n",
    "\n",
    "# for batch_idx,batch in enumerate(resized_val_loader):\n",
    "#     images = batch[\"image\"].to('cuda')\n",
    "#     images = torch.squeeze(images,dim=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Generate GradCAM 0,1,2 are classes \n",
    "#     cam0_enc12 = gcam_enc12(x=images, class_idx=0)\n",
    "#     cam1_enc12 = gcam_enc12(x=images, class_idx=1)\n",
    "#     cam2_enc12 = gcam_enc12(x=images, class_idx=2)\n",
    "    \n",
    "#     cam0_res12 = gcam_res12(x=images, class_idx=0)\n",
    "#     cam1_res12 = gcam_res12(x=images, class_idx=1)\n",
    "#     cam2_res12 = gcam_res12(x=images, class_idx=2)\n",
    "    \n",
    "#     cam0_enc3 = gcam_enc3(x=images, class_idx=0)\n",
    "#     cam1_enc3 = gcam_enc3(x=images, class_idx=1)\n",
    "#     cam2_enc3 = gcam_enc3(x=images, class_idx=2)\n",
    "    \n",
    "#     cam0_res3 = gcam_res3(x=images, class_idx=0)\n",
    "#     cam1_res3 = gcam_res3(x=images, class_idx=1)\n",
    "#     cam2_res3 = gcam_res3(x=images, class_idx=2)\n",
    "    \n",
    "#     cam0_enc5 = gcam_enc5(x=images, class_idx=0)\n",
    "#     cam1_enc5 = gcam_enc5(x=images, class_idx=1)\n",
    "#     cam2_enc5 = gcam_enc5(x=images, class_idx=2)\n",
    "    \n",
    "#     cam0_res5 = gcam_res5(x=images, class_idx=0)\n",
    "#     cam1_res5 = gcam_res5(x=images, class_idx=1)\n",
    "#     cam2_res5 = gcam_res5(x=images, class_idx=2)\n",
    "    \n",
    "#     cam_images = [cam0_enc12,cam1_enc12,cam2_enc12, \n",
    "#               cam0_res12,cam1_res12,cam2_res12,\n",
    "#               cam0_enc3,cam1_enc3,cam2_enc3,\n",
    "#               cam0_res3,cam1_res3,cam2_res3,\n",
    "#               cam0_enc5,cam1_enc5,cam2_enc5,\n",
    "#               cam0_res5,cam1_res5,cam2_res5]\n",
    "    \n",
    "    \n",
    "#     cam_image_names = [\"CAM0 ENC12\", \"CAM1 ENC12\", \"CAM2 ENC12\",\n",
    "#                        \"CAM0 RES12\", \"CAM1 RES12\", \"CAM2 RES12\",\n",
    "#                        \"CAM0 ENC3\", \"CAM1 ENC3\", \"CAM2 ENC3\",\n",
    "#                        \"CAM0 RES3\", \"CAM1 RES3\", \"CAM2 RES3\",\n",
    "#                        \"CAM0 ENC5\", \"CAM1 ENC5\", \"CAM2 ENC5\",\n",
    "#                        \"CAM0 RES5\", \"CAM1 RES5\", \"CAM2 RES5\", \n",
    "#                        \"NULL\", \"NULL\"]\n",
    "    \n",
    "    \n",
    "#     #print(cam.shape)\n",
    "    \n",
    "#     # Visualize the GradCAM results\n",
    "#     # We'll visualize the middle slice of each channel\n",
    "#     fig, axes = plt.subplots(2, 7, figsize=(20, 10))\n",
    "#     image_names = [\"flair\",\"t1\",\"t1ce\",\"t2\"]\n",
    "#     for channel in range(4):\n",
    "#         # Original image\n",
    "#         axes[0,channel].imshow(images[0, channel, :, :, 64].cpu().numpy(), cmap='gray')\n",
    "#         axes[0,channel].set_title(image_names[channel])\n",
    "#         axes[0,channel].axis('off')\n",
    "        \n",
    "    \n",
    "#     for i in range(4,7):\n",
    "#         axes[0,i].imshow(cam_images[-4+i][0, 0, :, :, 64].cpu().numpy())\n",
    "#         axes[0,i].set_title(cam_image_names[-4+i])\n",
    "#         axes[0,i].axis('off')\n",
    "        \n",
    "#     for channel in range(4):\n",
    "#         # Original image\n",
    "#         axes[1,channel].imshow(images[0, channel, :, :, 64].cpu().numpy(), cmap='gray')\n",
    "#         axes[1,channel].set_title(image_names[channel])\n",
    "#         axes[1,channel].axis('off')\n",
    "        \n",
    "    \n",
    "#     for i in range(7,10):\n",
    "#         axes[1,i-3].imshow(cam_images[-4+i][0, 0, :, :, 64].cpu().numpy())\n",
    "#         axes[1,i-3].set_title(cam_image_names[-4+i])\n",
    "#         axes[1,i-3].axis('off')\n",
    "        \n",
    "        \n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5bb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from monai.visualize import GradCAM\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming 'saved_model' is your loaded model\n",
    "# gcam = GradCAM(nn_module=saved_model.net, target_layers=\"out\")\n",
    "\n",
    "# for batch_idx,batch in enumerate(resized_val_loader):\n",
    "#     images = batch[\"image\"].to('cuda')\n",
    "#     images = torch.squeeze(images,dim=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Generate GradCAM\n",
    "#     cam0 = gcam(x=images, class_idx=0)\n",
    "#     cam1 = gcam(x=images, class_idx=1)\n",
    "#     cam2 = gcam(x=images, class_idx=2)\n",
    "    \n",
    "    \n",
    "#     print(cam.shape)\n",
    "    \n",
    "#     # Visualize the GradCAM results\n",
    "#     # We'll visualize the middle slice of each channel\n",
    "#     fig, axes = plt.subplots(1, 7, figsize=(20, 10))\n",
    "#     image_names = [\"flair\",\"t1\",\"t1ce\",\"t2\"]\n",
    "#     for channel in range(4):\n",
    "#         # Original image\n",
    "#         axes[channel].imshow(images[0, channel, :, :, 64].cpu().numpy(), cmap='gray')\n",
    "#         axes[channel].set_title(image_names[channel])\n",
    "#         axes[channel].axis('off')\n",
    "        \n",
    "#         # GradCAM\n",
    "#     axes[4].imshow(cam0[0, 0, :, :, 64].cpu().numpy())\n",
    "#     axes[4].set_title(f'GradCAM Class 0')\n",
    "#     axes[4].axis('off')\n",
    "    \n",
    "#     axes[5].imshow(cam1[0, 0, :, :, 64].cpu().numpy())\n",
    "#     axes[5].set_title(f'GradCAM Class 1')\n",
    "#     axes[5].axis('off')\n",
    "    \n",
    "#     axes[6].imshow(cam2[0, 0, :, :, 64].cpu().numpy())\n",
    "#     axes[6].set_title(f'GradCAM Class 2')\n",
    "#     axes[6].axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
